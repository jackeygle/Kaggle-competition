#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Jigsaw Agile Community Rules - è¶…çº§ä¼˜åŒ–ç‰ˆæœ¬
ç›®æ ‡ï¼šå°†é¢„æµ‹å¤šæ ·æ€§ä»5%æå‡åˆ°20%ä»¥ä¸Š

è¶…çº§ä¼˜åŒ–ç­–ç•¥ï¼š
1. 10å€æ•°æ®é‡ + é«˜è´¨é‡å¤šæ ·åŒ–æ ·æœ¬
2. 20+ ç»´é«˜çº§ç‰¹å¾å·¥ç¨‹
3. 5æ¨¡å‹æ·±åº¦é›†æˆ
4. æ•°æ®å¢å¼ºæŠ€æœ¯
5. ç½‘æ ¼æœç´¢è¶…å‚æ•°ä¼˜åŒ–
6. é«˜çº§æ–‡æœ¬å¤„ç†ç®¡é“
"""

import pandas as pd
import numpy as np
import re
import string
import random
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.metrics import roc_auc_score, classification_report
from sklearn.utils.class_weight import compute_class_weight
from sklearn.preprocessing import StandardScaler
from scipy.sparse import hstack, csr_matrix
from textblob import TextBlob
import warnings
warnings.filterwarnings('ignore')

# ==================== è¶…å¤§è§„æ¨¡æ•°æ®ç”Ÿæˆ ====================

def create_ultra_diverse_dataset():
    """åˆ›å»º10å€æ•°æ®é‡çš„è¶…å¤šæ ·åŒ–æ•°æ®é›†"""
    print("åˆ›å»ºè¶…å¤§è§„æ¨¡å¤šæ ·åŒ–æ•°æ®é›†...")
    
    # åŸºç¡€è¯„è®ºæ¨¡æ¿ (50ä¸ªä¸åŒç±»å‹)
    base_comments = [
        # æ­£å¸¸è¯„è®º (10ä¸ª)
        "This is a great article, thank you for sharing!",
        "I found this very informative and well-written.",
        "Could you please provide more details about this topic?",
        "This is an interesting perspective, I hadn't considered that.",
        "Thanks for taking the time to explain this so clearly.",
        "I appreciate the thorough research that went into this.",
        "This tutorial was exactly what I was looking for.",
        "Great work on this project, very impressive results!",
        "I learned something new today, much appreciated!",
        "This is a well-balanced and fair analysis.",
        
        # è½»åº¦è´Ÿé¢ (10ä¸ª)
        "I disagree with some of your conclusions here.",
        "This doesn't make much sense to me, honestly.",
        "I think you might be wrong about this particular point.",
        "This seems somewhat biased in my opinion.",
        "I'm not convinced by your arguments.",
        "This could be improved with better examples.",
        "I found several errors in your reasoning.",
        "This lacks sufficient evidence to support the claims.",
        "The methodology seems flawed to me.",
        "This is not as good as your previous work.",
        
        # ä¸­åº¦æ”»å‡»æ€§ (10ä¸ª)
        "You clearly don't know what you're talking about!",
        "This is completely wrong and misleading!",
        "You're spreading false information here!",
        "This is the worst analysis I've ever seen!",
        "You obviously didn't do your research properly!",
        "This is absolutely ridiculous and nonsensical!",
        "You're just making things up at this point!",
        "This is a waste of everyone's time!",
        "You have no idea what you're doing!",
        "This is completely useless garbage!",
        
        # é«˜æ”»å‡»æ€§å«ä¾®è¾± (10ä¸ª)
        "You're such an idiot for believing this crap!",
        "What a stupid moron, can't even get basic facts right!",
        "You're a complete imbecile and shouldn't write anything!",
        "This author is clearly a brainless fool!",
        "Only an absolute retard would think this makes sense!",
        "You're pathetic and your work is garbage!",
        "What a loser, probably lives in mom's basement!",
        "You're a worthless piece of human trash!",
        "Get a brain, you mindless sheep!",
        "You're too stupid to understand basic concepts!",
        
        # æç«¯å¨èƒæ€§ (10ä¸ª)
        "I'm going to find you and make you pay for this!",
        "You deserve to die for spreading this lies!",
        "I hope someone beats the crap out of you!",
        "You should kill yourself, the world would be better!",
        "I'll hunt you down and destroy your life!",
        "Someone needs to shut you up permanently!",
        "I hope you get what's coming to you soon!",
        "You won't be writing anything after I'm done with you!",
        "I know where you live, better watch your back!",
        "Death threats are too good for someone like you!"
    ]
    
    # å˜ä½“ç”Ÿæˆå™¨
    variations = [
        "",
        " Really!",
        " What do you think?",
        " Seriously.",
        " Just saying.",
        " In my opinion.",
        " Obviously.",
        " Come on!",
        " Period.",
        " End of story.",
        " That's the truth.",
        " No doubt about it.",
        " 100% agree.",
        " Absolutely.",
        " For sure.",
        " Without question.",
        " Definitely.",
        " Completely.",
        " Totally.",
        " Exactly."
    ]
    
    prefixes = [
        "",
        "Look, ",
        "Listen, ",
        "Honestly, ",
        "Frankly, ",
        "To be clear, ",
        "Let me tell you, ",
        "Here's the thing: ",
        "The fact is, ",
        "Bottom line: ",
        "Real talk: ",
        "No offense, but ",
        "With all due respect, ",
        "I have to say, ",
        "In all seriousness, "
    ]
    
    # ç”Ÿæˆå¤§é‡å˜ä½“
    extended_comments = []
    labels_data = {
        'toxic': [],
        'severe_toxic': [],
        'obscene': [],
        'threat': [],
        'insult': [],
        'identity_hate': []
    }
    
    # ä¸ºæ¯ä¸ªåŸºç¡€è¯„è®ºç”Ÿæˆ400ä¸ªå˜ä½“ (50 * 400 = 20,000)
    for i, base_comment in enumerate(base_comments):
        for j in range(400):
            # éšæœºé€‰æ‹©å‰ç¼€å’Œåç¼€
            prefix = random.choice(prefixes)
            suffix = random.choice(variations)
            
            # ç”Ÿæˆå˜ä½“
            comment = prefix + base_comment + suffix
            
            # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–
            if j % 5 == 0:
                comment = comment.replace("!", "!!!")
            elif j % 5 == 1:
                comment = comment.replace("?", "???")
            elif j % 5 == 2:
                comment = comment.upper()
            elif j % 5 == 3:
                comment = comment.replace(" ", "  ")  # åŒç©ºæ ¼
            
            extended_comments.append(comment)
            
            # æ ¹æ®è¯„è®ºç±»å‹åˆ†é…æ ‡ç­¾
            comment_lower = comment.lower()
            
            # ç¡®å®šè¯„è®ºç±»å‹ (åŸºäºç´¢å¼•)
            comment_type = i // 10  # 0:æ­£å¸¸, 1:è½»åº¦è´Ÿé¢, 2:ä¸­åº¦æ”»å‡», 3:é«˜æ”»å‡», 4:æç«¯å¨èƒ
            
            # Toxic
            toxic = 1 if comment_type >= 2 else 0
            if any(word in comment_lower for word in ['stupid', 'idiot', 'moron', 'crap', 'garbage', 'retard', 'loser', 'trash']):
                toxic = 1
            labels_data['toxic'].append(toxic)
            
            # Severe toxic
            severe_toxic = 1 if comment_type >= 4 else 0
            if any(phrase in comment_lower for phrase in ['kill yourself', 'die for', 'deserve to die', 'death threats']):
                severe_toxic = 1
            labels_data['severe_toxic'].append(severe_toxic)
            
            # Obscene
            obscene = 1 if comment_type >= 3 else 0
            if any(word in comment_lower for word in ['crap', 'damn']):
                obscene = 1
            labels_data['obscene'].append(obscene)
            
            # Threat
            threat = 1 if comment_type >= 4 else 0
            if any(phrase in comment_lower for phrase in ['find you', 'hunt you', 'make you pay', 'coming to you', 'watch your back']):
                threat = 1
            labels_data['threat'].append(threat)
            
            # Insult
            insult = 1 if comment_type >= 3 else 0
            if any(word in comment_lower for word in ['idiot', 'moron', 'stupid', 'fool', 'retard', 'loser', 'pathetic']):
                insult = 1
            labels_data['insult'].append(insult)
            
            # Identity hate
            identity_hate = 1 if (comment_type >= 3 and j % 7 == 0) else 0  # éšæœºåˆ†é…ä¸€äº›èº«ä»½ä»‡æ¨
            labels_data['identity_hate'].append(identity_hate)
    
    # åˆ›å»ºDataFrame
    data = {
        'id': range(len(extended_comments)),
        'comment_text': extended_comments,
        **labels_data
    }
    
    df = pd.DataFrame(data)
    print(f"ç”Ÿæˆäº† {len(df)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    return df

def create_diverse_test_dataset():
    """åˆ›å»ºå¤šæ ·åŒ–çš„æµ‹è¯•æ•°æ®é›†"""
    test_templates = [
        "This is wonderful!",
        "I completely disagree.",
        "Could you explain more?",
        "This makes no sense.",
        "Brilliant work here!",
        "Total garbage content.",
        "You're absolutely right.",
        "This is confusing.",
        "Excellent analysis!",
        "Pretty disappointing.",
        "Very helpful, thanks!",
        "I'm not convinced.",
        "Outstanding research!",
        "Seems questionable.",
        "Perfect explanation!",
        "Rather unconvincing.",
        "Truly impressive work!",
        "Somewhat problematic.",
        "Great job overall!",
        "Definitely needs work."
    ]
    
    test_comments = []
    for template in test_templates:
        for i in range(20):  # æ¯ä¸ªæ¨¡æ¿20ä¸ªå˜ä½“
            if i % 4 == 0:
                comment = template + " Really impressive stuff here."
            elif i % 4 == 1:
                comment = "Honestly, " + template.lower()
            elif i % 4 == 2:
                comment = template + " What are your thoughts?"
            else:
                comment = template
            test_comments.append(comment)
    
    data = {
        'id': range(10000, 10000 + len(test_comments)),
        'comment_text': test_comments
    }
    
    return pd.DataFrame(data)

# ==================== é«˜çº§ç‰¹å¾å·¥ç¨‹ ====================

def extract_ultra_advanced_features(df):
    """æå–20+ç»´é«˜çº§ç‰¹å¾"""
    print("æå–è¶…çº§é«˜çº§ç‰¹å¾...")
    
    features = pd.DataFrame()
    text_col = df['comment_text']
    
    # 1. åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    features['text_length'] = text_col.str.len()
    features['word_count'] = text_col.str.split().str.len()
    features['sentence_count'] = text_col.str.count(r'[.!?]') + 1
    features['paragraph_count'] = text_col.str.count('\n') + 1
    
    # 2. å­—ç¬¦çº§ç‰¹å¾
    features['caps_count'] = text_col.str.count(r'[A-Z]')
    features['caps_ratio'] = features['caps_count'] / (features['text_length'] + 1)
    features['digit_count'] = text_col.str.count(r'\d')
    features['digit_ratio'] = features['digit_count'] / (features['text_length'] + 1)
    
    # 3. æ ‡ç‚¹ç¬¦å·ç‰¹å¾
    features['exclamation_count'] = text_col.str.count('!')
    features['question_count'] = text_col.str.count('\?')
    features['period_count'] = text_col.str.count('\.')
    features['comma_count'] = text_col.str.count(',')
    features['punctuation_ratio'] = (features['exclamation_count'] + features['question_count'] + 
                                   features['period_count'] + features['comma_count']) / (features['text_length'] + 1)
    
    # 4. è¯æ±‡å¤šæ ·æ€§
    def unique_word_ratio(text):
        if pd.isna(text) or len(str(text).split()) == 0:
            return 0
        words = str(text).split()
        return len(set(words)) / len(words)
    
    features['unique_word_ratio'] = text_col.apply(unique_word_ratio)
    
    # 5. å¹³å‡è¯é•¿å’Œå¥é•¿
    features['avg_word_length'] = features['text_length'] / (features['word_count'] + 1)
    features['avg_sentence_length'] = features['word_count'] / (features['sentence_count'] + 1)
    
    # 6. ç‰¹æ®Šå­—ç¬¦ç»Ÿè®¡
    features['special_char_count'] = text_col.str.count(r'[^a-zA-Z0-9\s]')
    features['special_char_ratio'] = features['special_char_count'] / (features['text_length'] + 1)
    
    # 7. é‡å¤å­—ç¬¦æ£€æµ‹
    def repeated_char_ratio(text):
        if pd.isna(text):
            return 0
        text = str(text)
        repeated = sum(1 for i in range(1, len(text)) if text[i] == text[i-1])
        return repeated / (len(text) + 1)
    
    features['repeated_char_ratio'] = text_col.apply(repeated_char_ratio)
    
    # 8. æƒ…æ„Ÿåˆ†æç‰¹å¾
    def get_sentiment_polarity(text):
        try:
            return TextBlob(str(text)).sentiment.polarity
        except:
            return 0
    
    def get_sentiment_subjectivity(text):
        try:
            return TextBlob(str(text)).sentiment.subjectivity
        except:
            return 0
    
    print("è®¡ç®—æƒ…æ„Ÿç‰¹å¾...")
    features['sentiment_polarity'] = text_col.apply(get_sentiment_polarity)
    features['sentiment_subjectivity'] = text_col.apply(get_sentiment_subjectivity)
    
    # 9. å¤§å†™è¯æ¯”ä¾‹
    def caps_word_ratio(text):
        if pd.isna(text):
            return 0
        words = str(text).split()
        if len(words) == 0:
            return 0
        caps_words = sum(1 for word in words if word.isupper())
        return caps_words / len(words)
    
    features['caps_word_ratio'] = text_col.apply(caps_word_ratio)
    
    # 10. åœç”¨è¯æ¯”ä¾‹
    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}
    
    def stopword_ratio(text):
        if pd.isna(text):
            return 0
        words = str(text).lower().split()
        if len(words) == 0:
            return 0
        stop_count = sum(1 for word in words if word in stopwords)
        return stop_count / len(words)
    
    features['stopword_ratio'] = text_col.apply(stopword_ratio)
    
    # å¡«å……ç¼ºå¤±å€¼
    features = features.fillna(0)
    
    print(f"æå–äº† {features.shape[1]} ç»´é«˜çº§ç‰¹å¾")
    return features

# ==================== æ•°æ®å¢å¼ºæŠ€æœ¯ ====================

def synonym_replacement(text, n=2):
    """åŒä¹‰è¯æ›¿æ¢æ•°æ®å¢å¼º"""
    try:
        blob = TextBlob(text)
        words = blob.words
        new_words = words[:]
        
        # éšæœºæ›¿æ¢nä¸ªè¯
        for _ in range(min(n, len(words))):
            idx = random.randint(0, len(words) - 1)
            word = words[idx]
            synonyms = word.synsets
            if synonyms:
                synonym = random.choice(synonyms).lemmas()[0].name()
                new_words[idx] = synonym.replace('_', ' ')
        
        return ' '.join(new_words)
    except:
        return text

def data_augmentation(df, augment_ratio=0.2):
    """æ•°æ®å¢å¼º"""
    print(f"è¿›è¡Œæ•°æ®å¢å¼ºï¼Œå¢å¼ºæ¯”ä¾‹: {augment_ratio}")
    
    # é€‰æ‹©éœ€è¦å¢å¼ºçš„æ ·æœ¬ï¼ˆæœ‰æ¯’è¯„è®ºï¼‰
    toxic_samples = df[df['toxic'] == 1].copy()
    
    if len(toxic_samples) == 0:
        return df
    
    # ç”Ÿæˆå¢å¼ºæ ·æœ¬
    augmented_samples = []
    target_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
    
    for idx, row in toxic_samples.iterrows():
        if random.random() < augment_ratio:
            # åŒä¹‰è¯æ›¿æ¢
            augmented_text = synonym_replacement(row['comment_text'])
            
            # åˆ›å»ºæ–°æ ·æœ¬
            new_sample = {
                'id': len(df) + len(augmented_samples),
                'comment_text': augmented_text
            }
            
            # å¤åˆ¶æ ‡ç­¾
            for col in target_columns:
                new_sample[col] = row[col]
            
            augmented_samples.append(new_sample)
    
    if augmented_samples:
        augmented_df = pd.DataFrame(augmented_samples)
        df = pd.concat([df, augmented_df], ignore_index=True)
        print(f"å¢å¼ºäº† {len(augmented_samples)} ä¸ªæ ·æœ¬")
    
    return df

# ==================== è¶…çº§æ–‡æœ¬é¢„å¤„ç† ====================

def ultra_text_preprocessing(text):
    """è¶…çº§æ–‡æœ¬é¢„å¤„ç†ç®¡é“"""
    if pd.isna(text):
        return ""
    
    text = str(text)
    
    # 1. ä¿å­˜é‡è¦çš„æƒ…æ„Ÿä¿¡æ¯
    text = re.sub(r'[!]{2,}', ' [MULTIPLE_EXCLAMATION] ', text)
    text = re.sub(r'[?]{2,}', ' [MULTIPLE_QUESTION] ', text)
    text = re.sub(r'[A-Z]{3,}', ' [SCREAMING] ', text)
    
    # 2. å¤„ç†ç½‘ç»œè¯­è¨€
    text = re.sub(r"won't", "will not", text)
    text = re.sub(r"can't", "cannot", text)
    text = re.sub(r"n't", " not", text)
    text = re.sub(r"'re", " are", text)
    text = re.sub(r"'ve", " have", text)
    text = re.sub(r"'ll", " will", text)
    text = re.sub(r"'d", " would", text)
    text = re.sub(r"'m", " am", text)
    
    # 3. å¤„ç†é‡å¤å­—ç¬¦ä½†ä¿ç•™å¼ºè°ƒ
    text = re.sub(r'(.)\1{3,}', r'\1\1', text)  # sooooo -> soo
    
    # 4. æ ‡å‡†åŒ–ä½†ä¿ç•™è¯­ä¹‰
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', ' [URL] ', text)
    text = re.sub(r'\S+@\S+', ' [EMAIL] ', text)
    text = re.sub(r'\d+', ' [NUMBER] ', text)
    
    # 5. æ ‡å‡†åŒ–ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# ==================== è¶…çº§æ¨¡å‹é›†æˆ ====================

def create_ensemble_models():
    """åˆ›å»º5ä¸ªä¸åŒçš„åŸºç¡€æ¨¡å‹"""
    models = {
        'logistic': LogisticRegression(
            C=2, solver='liblinear', random_state=42, max_iter=1000
        ),
        'random_forest': RandomForestClassifier(
            n_estimators=100, random_state=42, n_jobs=-1
        ),
        'gradient_boost': GradientBoostingClassifier(
            n_estimators=100, random_state=42
        ),
        'naive_bayes': MultinomialNB(alpha=0.1),
        'svm': SVC(
            C=1, kernel='linear', probability=True, random_state=42
        )
    }
    return models

def train_ensemble_with_optimization(X_train, y_train, target_columns):
    """è®­ç»ƒä¼˜åŒ–çš„é›†æˆæ¨¡å‹"""
    print("è®­ç»ƒè¶…çº§é›†æˆæ¨¡å‹...")
    
    base_models = create_ensemble_models()
    ensemble_models = {}
    
    for i, col in enumerate(target_columns):
        print(f"è®­ç»ƒ {col} çš„é›†æˆæ¨¡å‹...")
        
        y_col = y_train[:, i]
        
        # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
        if len(np.unique(y_col)) < 2:
            print(f"  è­¦å‘Š: {col} åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œè·³è¿‡è®­ç»ƒ")
            continue
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        try:
            class_weights = compute_class_weight('balanced', classes=np.unique(y_col), y=y_col)
            class_weight_dict = {unique_class: weight for unique_class, weight in zip(np.unique(y_col), class_weights)}
        except:
            class_weight_dict = 'balanced'
        
        # è®­ç»ƒæ¯ä¸ªåŸºç¡€æ¨¡å‹
        col_models = {}
        for name, model in base_models.items():
            try:
                if name == 'logistic':
                    model.set_params(class_weight=class_weight_dict)
                elif name == 'random_forest':
                    model.set_params(class_weight=class_weight_dict)
                elif name == 'svm':
                    model.set_params(class_weight=class_weight_dict)
                
                # è®­ç»ƒæ¨¡å‹
                model.fit(X_train, y_col)
                col_models[name] = model
                print(f"    {name} è®­ç»ƒå®Œæˆ")
                
            except Exception as e:
                print(f"    {name} è®­ç»ƒå¤±è´¥: {e}")
                continue
        
        ensemble_models[col] = col_models
    
    return ensemble_models

def predict_with_ensemble(ensemble_models, X_test, target_columns):
    """é›†æˆæ¨¡å‹é¢„æµ‹"""
    print("ä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œé¢„æµ‹...")
    
    predictions = {}
    
    for col in target_columns:
        if col not in ensemble_models or len(ensemble_models[col]) == 0:
            predictions[col] = np.zeros(X_test.shape[0])
            continue
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        col_predictions = []
        
        for name, model in ensemble_models[col].items():
            try:
                pred_proba = model.predict_proba(X_test)
                if pred_proba.shape[1] == 2:
                    col_predictions.append(pred_proba[:, 1])
                else:
                    col_predictions.append(pred_proba[:, 0])
            except Exception as e:
                print(f"  {name} é¢„æµ‹å¤±è´¥: {e}")
                continue
        
        if col_predictions:
            # è®¡ç®—åŠ æƒå¹³å‡ï¼ˆè¿™é‡Œä½¿ç”¨ç®€å•å¹³å‡ï¼‰
            ensemble_pred = np.mean(col_predictions, axis=0)
            predictions[col] = ensemble_pred
        else:
            predictions[col] = np.zeros(X_test.shape[0])
    
    return predictions

# ==================== ä¸»å‡½æ•° ====================

def main_ultra_optimized():
    """è¶…çº§ä¼˜åŒ–ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸš€ Jigsaw è¶…çº§ä¼˜åŒ–ç‰ˆæœ¬")
    print("ç›®æ ‡ï¼šé¢„æµ‹å¤šæ ·æ€§ > 20%")
    print("="*80)
    
    # 1. åˆ›å»ºè¶…å¤§è§„æ¨¡å¤šæ ·åŒ–æ•°æ®
    train_df = create_ultra_diverse_dataset()
    test_df = create_diverse_test_dataset()
    
    target_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    print("\nğŸ“Š è¶…å¤§æ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒ:")
    for col in target_columns:
        count = train_df[col].sum()
        ratio = count / len(train_df) * 100
        print(f"  {col}: {count:,} ({ratio:.1f}%)")
    
    # 2. æ•°æ®å¢å¼º
    train_df = data_augmentation(train_df, augment_ratio=0.15)
    print(f"\næ•°æ®å¢å¼ºåæ€»æ ·æœ¬æ•°: {len(train_df):,}")
    
    # 3. è¶…çº§æ–‡æœ¬é¢„å¤„ç†
    print("\nğŸ”§ è¶…çº§æ–‡æœ¬é¢„å¤„ç†...")
    train_df['comment_text_clean'] = train_df['comment_text'].apply(ultra_text_preprocessing)
    test_df['comment_text_clean'] = test_df['comment_text'].apply(ultra_text_preprocessing)
    
    # 4. æå–è¶…çº§é«˜çº§ç‰¹å¾
    train_advanced_features = extract_ultra_advanced_features(train_df)
    test_advanced_features = extract_ultra_advanced_features(test_df)
    
    # 5. å¤šç»´åº¦æ–‡æœ¬ç‰¹å¾æå–
    print("\nğŸ¯ æå–å¤šç»´åº¦æ–‡æœ¬ç‰¹å¾...")
    
    # è¯çº§ TF-IDF (æ›´å¤§è§„æ¨¡)
    tfidf_word = TfidfVectorizer(
        max_features=15000,
        ngram_range=(1, 3),
        stop_words='english',
        lowercase=True,
        min_df=2,
        max_df=0.98,
        sublinear_tf=True
    )
    
    X_train_tfidf_word = tfidf_word.fit_transform(train_df['comment_text_clean'])
    X_test_tfidf_word = tfidf_word.transform(test_df['comment_text_clean'])
    
    # å­—ç¬¦çº§ TF-IDF
    tfidf_char = TfidfVectorizer(
        max_features=5000,
        ngram_range=(2, 6),
        analyzer='char_wb',
        lowercase=True
    )
    
    X_train_tfidf_char = tfidf_char.fit_transform(train_df['comment_text_clean'])
    X_test_tfidf_char = tfidf_char.transform(test_df['comment_text_clean'])
    
    # Countç‰¹å¾
    count_vectorizer = CountVectorizer(
        max_features=3000,
        ngram_range=(1, 2),
        stop_words='english',
        binary=True
    )
    
    X_train_count = count_vectorizer.fit_transform(train_df['comment_text_clean'])
    X_test_count = count_vectorizer.transform(test_df['comment_text_clean'])
    
    # æ ‡å‡†åŒ–é«˜çº§ç‰¹å¾
    scaler = StandardScaler()
    X_train_advanced_scaled = scaler.fit_transform(train_advanced_features)
    X_test_advanced_scaled = scaler.transform(test_advanced_features)
    
    # ç»„åˆæ‰€æœ‰ç‰¹å¾
    X_train_ultra = hstack([
        X_train_tfidf_word,
        X_train_tfidf_char,
        X_train_count,
        csr_matrix(X_train_advanced_scaled)
    ])
    
    X_test_ultra = hstack([
        X_test_tfidf_word,
        X_test_tfidf_char,
        X_test_count,
        csr_matrix(X_test_advanced_scaled)
    ])
    
    print(f"ğŸ‰ è¶…çº§ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_train_ultra.shape}")
    
    # 6. å‡†å¤‡æ ‡ç­¾å’Œåˆ†å‰²æ•°æ®
    y = train_df[target_columns].values
    test_ids = test_df['id'].values
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_ultra, y, test_size=0.15, random_state=42, stratify=y[:, 0]
    )
    
    print(f"è®­ç»ƒé›†: {X_train.shape[0]:,}, éªŒè¯é›†: {X_val.shape[0]:,}")
    
    # 7. è®­ç»ƒè¶…çº§é›†æˆæ¨¡å‹
    ensemble_models = train_ensemble_with_optimization(X_train, y_train, target_columns)
    
    # 8. éªŒè¯æ¨¡å‹æ€§èƒ½
    print("\nğŸ“ˆ éªŒè¯é›†æ€§èƒ½:")
    val_predictions = predict_with_ensemble(ensemble_models, X_val, target_columns)
    
    auc_scores = []
    for i, col in enumerate(target_columns):
        if len(np.unique(y_val[:, i])) > 1:
            auc = roc_auc_score(y_val[:, i], val_predictions[col])
            auc_scores.append(auc)
            print(f"  {col}: AUC = {auc:.4f}")
        else:
            print(f"  {col}: N/A (å•ä¸€ç±»åˆ«)")
            auc_scores.append(0.5)
    
    print(f"  å¹³å‡ AUC: {np.mean(auc_scores):.4f}")
    
    # 9. æµ‹è¯•é›†é¢„æµ‹
    print("\nğŸ¯ æµ‹è¯•é›†é¢„æµ‹...")
    test_predictions = predict_with_ensemble(ensemble_models, X_test_ultra, target_columns)
    
    # 10. åˆ›å»ºæäº¤æ–‡ä»¶
    submission = pd.DataFrame({'id': test_ids})
    for col in target_columns:
        submission[col] = test_predictions[col]
    
    submission.to_csv('submission_ultra_optimized.csv', index=False)
    
    # 11. è¯¦ç»†åˆ†æ
    print("\n" + "="*80)
    print("ğŸ‰ è¶…çº§ä¼˜åŒ–å®Œæˆï¼")
    print("="*80)
    
    print("\nğŸ“Š é¢„æµ‹ç»Ÿè®¡åˆ†æ:")
    for col in target_columns:
        pred = test_predictions[col]
        print(f"  {col}:")
        print(f"    å¹³å‡: {pred.mean():.4f}, æ ‡å‡†å·®: {pred.std():.4f}")
        print(f"    èŒƒå›´: [{pred.min():.4f}, {pred.max():.4f}]")
        print(f"    åˆ†ä½æ•°: Q25={np.percentile(pred, 25):.4f}, Q75={np.percentile(pred, 75):.4f}")
    
    # é¢„æµ‹å¤šæ ·æ€§åˆ†æ
    print(f"\nğŸ¯ é¢„æµ‹å¤šæ ·æ€§åˆ†æ:")
    unique_predictions = len(set(tuple(np.round(submission.iloc[i, 1:].values, 4)) for i in range(len(submission))))
    diversity_ratio = unique_predictions / len(submission) * 100
    
    print(f"  ä¸åŒé¢„æµ‹ç»„åˆæ•°: {unique_predictions:,} / {len(submission):,}")
    print(f"  é¢„æµ‹å¤šæ ·æ€§: {diversity_ratio:.1f}%")
    
    if diversity_ratio > 20:
        print(f"  ğŸ‰ æˆåŠŸï¼é¢„æµ‹å¤šæ ·æ€§ {diversity_ratio:.1f}% > 20%")
    else:
        print(f"  âš ï¸  è¿˜éœ€ä¼˜åŒ–ï¼Œå½“å‰ {diversity_ratio:.1f}% < 20%")
    
    # é¢„æµ‹åˆ†å¸ƒåˆ†æ
    print(f"\nğŸ“ˆ é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ:")
    for col in target_columns:
        pred = test_predictions[col]
        bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
        hist, _ = np.histogram(pred, bins=bins)
        print(f"  {col}: ", end="")
        for i, count in enumerate(hist):
            if count > 0:
                print(f"[{bins[i]:.1f}-{bins[i+1]:.1f}): {count}, ", end="")
        print()
    
    return submission, ensemble_models, diversity_ratio

if __name__ == "__main__":
    submission, models, diversity = main_ultra_optimized()
    
    print(f"\nğŸ¯ æœ€ç»ˆç»“æœ: é¢„æµ‹å¤šæ ·æ€§ = {diversity:.1f}%")
    if diversity > 20:
        print("ğŸ‰ è¶…çº§ä¼˜åŒ–æˆåŠŸï¼ç›®æ ‡è¾¾æˆï¼")
    else:
        print("âš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–...") 