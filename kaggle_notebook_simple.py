# ==================================================
# Jigsaw Agile Community Rules - Kaggle Notebook ç‰ˆæœ¬
# ç®€å•çš„ TF-IDF + é€»è¾‘å›å½’å¤šæ ‡ç­¾åˆ†ç±»æ¨¡å‹
# ==================================================

import pandas as pd
import numpy as np
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import warnings
warnings.filterwarnings('ignore')

print("å¼€å§‹è®­ç»ƒ Jigsaw æ¯’æ€§è¯„è®ºåˆ†ç±»æ¨¡å‹...")

# ==================== 1. æ•°æ®åŠ è½½ ====================
print("1. æ­£åœ¨åŠ è½½æ•°æ®...")

# è¯»å–æ•°æ®æ–‡ä»¶
train_df = pd.read_csv('../input/jigsaw-agile-community-rules/train.csv')
test_df = pd.read_csv('../input/jigsaw-agile-community-rules/test.csv')
sample_submission = pd.read_csv('../input/jigsaw-agile-community-rules/sample_submission.csv')

print(f"è®­ç»ƒæ•°æ®: {train_df.shape}")
print(f"æµ‹è¯•æ•°æ®: {test_df.shape}")
print(f"è®­ç»ƒæ•°æ®åˆ—: {list(train_df.columns)}")

# ==================== 2. æ–‡æœ¬æ¸…æ´— ====================
print("2. æ­£åœ¨æ¸…æ´—æ–‡æœ¬æ•°æ®...")

def clean_text(text):
    """ç®€å•çš„æ–‡æœ¬æ¸…æ´—"""
    if pd.isna(text):
        return ""
    
    text = str(text).lower()  # è½¬å°å†™
    text = re.sub(r'<[^>]+>', '', text)  # å»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'[^a-zA-Z\s\.\!\?\,]', ' ', text)  # ä¿ç•™å­—æ¯å’ŒåŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'\s+', ' ', text).strip()  # æ ‡å‡†åŒ–ç©ºæ ¼
    
    return text

# åº”ç”¨æ–‡æœ¬æ¸…æ´—
train_df['comment_text_clean'] = train_df['comment_text'].apply(clean_text)
test_df['comment_text_clean'] = test_df['comment_text'].apply(clean_text)

# ==================== 3. å‡†å¤‡æ•°æ® ====================
print("3. æ­£åœ¨å‡†å¤‡è®­ç»ƒæ•°æ®...")

# ç¡®å®šç›®æ ‡åˆ—ï¼ˆæ ¹æ®sample_submissionçš„åˆ—ç¡®å®šï¼‰
target_cols = [col for col in sample_submission.columns if col != 'id']
print(f"ç›®æ ‡æ ‡ç­¾: {target_cols}")

# å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
X_text = train_df['comment_text_clean'].fillna("")
y = train_df[target_cols].values

test_text = test_df['comment_text_clean'].fillna("")
test_ids = test_df['id'].values

# åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
X_train_text, X_val_text, y_train, y_val = train_test_split(
    X_text, y, test_size=0.2, random_state=42
)

print(f"è®­ç»ƒé›†å¤§å°: {len(X_train_text)}")
print(f"éªŒè¯é›†å¤§å°: {len(X_val_text)}")

# ==================== 4. TF-IDF ç‰¹å¾æå– ====================
print("4. æ­£åœ¨æå– TF-IDF ç‰¹å¾...")

# åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨
vectorizer = TfidfVectorizer(
    max_features=10000,        # é™åˆ¶ç‰¹å¾æ•°é‡
    ngram_range=(1, 2),        # ä½¿ç”¨1-gramå’Œ2-gram
    stop_words='english',      # å»é™¤è‹±æ–‡åœç”¨è¯
    lowercase=True,
    strip_accents='ascii'
)

# æ‹Ÿåˆå¹¶è½¬æ¢è®­ç»ƒæ•°æ®
X_train_tfidf = vectorizer.fit_transform(X_train_text)
X_val_tfidf = vectorizer.transform(X_val_text)
X_test_tfidf = vectorizer.transform(test_text)

print(f"TF-IDF ç‰¹å¾å½¢çŠ¶: {X_train_tfidf.shape}")

# ==================== 5. æ¨¡å‹è®­ç»ƒ ====================
print("5. æ­£åœ¨è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...")

# åˆ›å»ºå¤šè¾“å‡ºé€»è¾‘å›å½’æ¨¡å‹
base_model = LogisticRegression(
    C=4,                       # æ­£åˆ™åŒ–å‚æ•°
    solver='liblinear',        # æ±‚è§£å™¨
    random_state=42,
    max_iter=1000
)

# å¤šæ ‡ç­¾åˆ†ç±»å™¨
model = MultiOutputClassifier(base_model, n_jobs=-1)

# è®­ç»ƒæ¨¡å‹
model.fit(X_train_tfidf, y_train)

print("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

# ==================== 6. æ¨¡å‹éªŒè¯ ====================
print("6. æ­£åœ¨éªŒè¯æ¨¡å‹æ€§èƒ½...")

# é¢„æµ‹éªŒè¯é›†
y_val_pred = model.predict_proba(X_val_tfidf)

# è®¡ç®—AUCåˆ†æ•°
auc_scores = []
for i, col in enumerate(target_cols):
    if len(model.estimators_[i].classes_) == 2:
        y_pred_prob = y_val_pred[i][:, 1]  # è·å–æ­£ç±»æ¦‚ç‡
    else:
        y_pred_prob = y_val_pred[i][:, 0]  # å¦‚æœåªæœ‰ä¸€ä¸ªç±»åˆ«
    
    auc = roc_auc_score(y_val[:, i], y_pred_prob)
    auc_scores.append(auc)
    print(f"{col} AUC: {auc:.4f}")

print(f"å¹³å‡ AUC: {np.mean(auc_scores):.4f}")

# ==================== 7. æµ‹è¯•é›†é¢„æµ‹ ====================
print("7. æ­£åœ¨å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹...")

# é¢„æµ‹æµ‹è¯•é›†
y_test_pred = model.predict_proba(X_test_tfidf)

# æ•´ç†é¢„æµ‹ç»“æœ
predictions = {'id': test_ids}
for i, col in enumerate(target_cols):
    if len(model.estimators_[i].classes_) == 2:
        predictions[col] = y_test_pred[i][:, 1]
    else:
        predictions[col] = np.zeros(len(test_ids))

# ==================== 8. åˆ›å»ºæäº¤æ–‡ä»¶ ====================
print("8. æ­£åœ¨åˆ›å»ºæäº¤æ–‡ä»¶...")

# åˆ›å»ºæäº¤DataFrame
submission = pd.DataFrame(predictions)

# ä¿å­˜æ–‡ä»¶
submission.to_csv('submission.csv', index=False)

print("æäº¤æ–‡ä»¶å·²ä¿å­˜ï¼")
print("\næäº¤æ–‡ä»¶é¢„è§ˆ:")
print(submission.head())
print(f"\næäº¤æ–‡ä»¶å½¢çŠ¶: {submission.shape}")

# æ˜¾ç¤ºé¢„æµ‹ç»Ÿè®¡
print("\né¢„æµ‹æ¦‚ç‡ç»Ÿè®¡:")
for col in target_cols:
    print(f"{col}: å¹³å‡={predictions[col].mean():.4f}, æœ€å¤§={predictions[col].max():.4f}")

print("\nğŸ‰ æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹å®Œæˆï¼å¯ä»¥ä¸‹è½½ submission.csv æ–‡ä»¶æäº¤äº†ï¼")

# ==================================================
# å¯é€‰ï¼šæ·»åŠ ç®€å•çš„æ•°æ®åˆ†æ
# ==================================================
print("\n" + "="*50)
print("æ•°æ®åˆ†æ")
print("="*50)

# æ ‡ç­¾åˆ†å¸ƒ
print("è®­ç»ƒæ•°æ®æ ‡ç­¾åˆ†å¸ƒ:")
for col in target_cols:
    count = train_df[col].sum()
    ratio = count / len(train_df)
    print(f"{col}: {count} ({ratio:.1%})")

# æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
text_lengths = train_df['comment_text'].str.len()
print(f"\næ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
print(f"å¹³å‡é•¿åº¦: {text_lengths.mean():.1f}")
print(f"ä¸­ä½æ•°é•¿åº¦: {text_lengths.median():.1f}")
print(f"æœ€å¤§é•¿åº¦: {text_lengths.max()}")

print("\næ¨¡å‹è®­ç»ƒå®Œæˆï¼ğŸš€") 