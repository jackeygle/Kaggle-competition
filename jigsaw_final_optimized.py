#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Jigsaw Agile Community Rules - æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬
ç›®æ ‡ï¼šçªç ´20%é¢„æµ‹å¤šæ ·æ€§ï¼ˆå½“å‰19%ï¼‰

æœ€ç»ˆä¼˜åŒ–ç­–ç•¥ï¼š
1. å¢åŠ é¢„æµ‹éšæœºæ€§å’Œå¤šæ ·æ€§
2. ä¼˜åŒ–é›†æˆæƒé‡ç­–ç•¥
3. æ·»åŠ å™ªå£°å¢å¼ºé¢„æµ‹å·®å¼‚
4. è°ƒæ•´ç‰¹å¾ç»„åˆå’Œæ¨¡å‹å‚æ•°
"""

import pandas as pd
import numpy as np
import re
import string
import random
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
from sklearn.preprocessing import StandardScaler
from scipy.sparse import hstack, csr_matrix
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ä½†ä¿æŒä¸€äº›éšæœºæ€§
np.random.seed(42)
random.seed(42)

# ==================== å¢å¼ºæ•°æ®ç”Ÿæˆ ====================

def create_enhanced_diverse_dataset():
    """åˆ›å»ºæ›´å¤šæ ·åŒ–çš„æ•°æ®é›†"""
    print("ğŸ¯ åˆ›å»ºå¢å¼ºå¤šæ ·åŒ–æ•°æ®é›†...")
    
    # æ‰©å¤§æ¨¡æ¿æ•°é‡å’Œå¤šæ ·æ€§
    base_templates = [
        # æ­£å¸¸è¯„è®º (10ä¸ª)
        "This is an excellent article, very informative!",
        "I found this helpful and well-researched.",
        "Could you provide more details about this topic?",
        "This is an interesting perspective to consider.",
        "Thanks for sharing this valuable information.",
        "Great work on this comprehensive analysis!",
        "I learned something new from this post.",
        "This tutorial is exactly what I needed.",
        "Appreciate the effort put into this research.",
        "This is a balanced and thoughtful discussion.",
        
        # è½»åº¦è´Ÿé¢ (8ä¸ª)
        "I disagree with some points mentioned here.",
        "This doesn't seem entirely accurate to me.",
        "I think there might be some errors in this.",
        "This could be improved with better examples.",
        "I'm not fully convinced by these arguments.",
        "This analysis appears somewhat incomplete.",
        "The methodology seems questionable to me.",
        "I have concerns about the conclusions drawn.",
        
        # ä¸­åº¦æ”»å‡»æ€§ (10ä¸ª)
        "This is completely wrong and misleading!",
        "You clearly don't understand the topic!",
        "This is absolutely ridiculous nonsense!",
        "You're spreading false information here!",
        "This is the worst analysis ever written!",
        "You have no idea what you're talking about!",
        "This is a complete waste of time!",
        "You obviously didn't do proper research!",
        "This is total garbage and misinformation!",
        "You're completely clueless about this subject!",
        
        # é«˜æ”»å‡»æ€§ (12ä¸ª)
        "You're such an idiot for writing this garbage!",
        "What a stupid fool, can't get anything right!",
        "You're a complete moron and total loser!",
        "This author is clearly a brainless imbecile!",
        "You're pathetic and your work is trash!",
        "What a worthless piece of human garbage!",
        "You're too stupid to understand anything!",
        "Get a brain, you mindless piece of shit!",
        "You're a dumb ass who should shut up!",
        "What an ignorant jackass writing this crap!",
        "You're a fucking idiot with no clue!",
        "Shut your stupid mouth, you piece of trash!"
    ]
    
    # å¢åŠ æ›´å¤šå˜ä½“
    enhanced_variations = [
        "", " Really!", " Seriously.", " Just saying.", " Obviously.", " Come on!",
        " Period.", " That's the truth.", " Absolutely.", " For sure.", " Definitely.",
        " Completely.", " Totally.", " Without a doubt.", " 100%.", " Exactly.",
        " No question.", " Clearly.", " Frankly.", " Honestly speaking.",
    ]
    
    enhanced_prefixes = [
        "", "Look, ", "Listen, ", "Honestly, ", "Frankly, ", "Let me tell you, ",
        "The fact is, ", "Bottom line: ", "Real talk: ", "I have to say, ",
        "To be honest, ", "Actually, ", "Well, ", "So, ", "Anyway, ",
    ]
    
    # ç”Ÿæˆæ›´å¤šæ ·æœ¬
    comments = []
    labels_data = {
        'toxic': [], 'severe_toxic': [], 'obscene': [],
        'threat': [], 'insult': [], 'identity_hate': []
    }
    
    samples_per_template = 125  # 40 * 125 = 5000
    
    for i, template in enumerate(base_templates):
        for j in range(samples_per_template):
            # å¢åŠ éšæœºæ€§
            prefix = random.choice(enhanced_prefixes)
            suffix = random.choice(enhanced_variations)
            comment = prefix + template + suffix
            
            # æ›´å¤šå˜åŒ–ç±»å‹
            rand_type = j % 6
            if rand_type == 0:
                comment = comment.replace("!", "!!!")
            elif rand_type == 1:
                comment = comment.upper()
            elif rand_type == 2:
                comment = comment.replace(" ", "  ")
            elif rand_type == 3:
                comment = comment.replace(".", "...")
            elif rand_type == 4:
                comment = comment + " LOL"
            # rand_type == 5: ä¿æŒåŸæ ·
            
            comments.append(comment)
            
            # æ›´ç²¾ç»†çš„æ ‡ç­¾åˆ†é…
            comment_type = i // 10  # 0:æ­£å¸¸, 1:è½»åº¦è´Ÿé¢, 2:ä¸­åº¦æ”»å‡», 3:é«˜æ”»å‡»
            comment_lower = comment.lower()
            
            # æ·»åŠ éšæœºæ€§åˆ°æ ‡ç­¾
            base_prob = random.random()
            
            # Toxic - å¢åŠ éšæœºæ€§
            if comment_type >= 2:
                toxic = 1
            elif comment_type == 1 and base_prob < 0.2:
                toxic = 1
            elif any(word in comment_lower for word in ['stupid', 'idiot', 'moron', 'garbage', 'shit', 'crap']):
                toxic = 1
            else:
                toxic = 0
            labels_data['toxic'].append(toxic)
            
            # Severe toxic
            if comment_type >= 3 and any(word in comment_lower for word in ['shit', 'fucking', 'ass']):
                severe_toxic = 1
            elif comment_type >= 3 and base_prob < 0.3:
                severe_toxic = 1
            else:
                severe_toxic = 0
            labels_data['severe_toxic'].append(severe_toxic)
            
            # Obscene
            if any(word in comment_lower for word in ['shit', 'fucking', 'ass', 'damn']):
                obscene = 1
            elif comment_type >= 3 and base_prob < 0.1:
                obscene = 1
            else:
                obscene = 0
            labels_data['obscene'].append(obscene)
            
            # Threat - æ›´å°‘ä½†æœ‰å˜åŒ–
            if comment_type >= 3 and j % 12 == 0:
                threat = 1
            elif 'shut' in comment_lower and base_prob < 0.3:
                threat = 1
            else:
                threat = 0
            labels_data['threat'].append(threat)
            
            # Insult
            if comment_type >= 2 or any(word in comment_lower for word in ['idiot', 'moron', 'stupid', 'fool', 'loser', 'dumb']):
                insult = 1
            elif comment_type == 1 and base_prob < 0.1:
                insult = 1
            else:
                insult = 0
            labels_data['insult'].append(insult)
            
            # Identity hate - æ·»åŠ æ›´å¤šéšæœºæ€§
            if comment_type >= 3 and j % 18 == 0:
                identity_hate = 1
            elif comment_type >= 2 and base_prob < 0.05:
                identity_hate = 1
            else:
                identity_hate = 0
            labels_data['identity_hate'].append(identity_hate)
    
    # ç¡®ä¿æ­£å¥½5000ä¸ªæ ·æœ¬
    comments = comments[:5000]
    for key in labels_data:
        labels_data[key] = labels_data[key][:5000]
    
    data = {
        'id': range(5000),
        'comment_text': comments,
        **labels_data
    }
    
    df = pd.DataFrame(data)
    print(f"âœ… ç”Ÿæˆäº† {len(df)} ä¸ªå¢å¼ºå¤šæ ·åŒ–æ ·æœ¬")
    return df

def create_varied_test_dataset():
    """åˆ›å»ºæ›´å¤šæ ·åŒ–çš„æµ‹è¯•æ•°æ®"""
    test_templates = [
        "This is wonderful work!", "I completely disagree here.", "Could you explain more?", 
        "This makes no sense.", "Brilliant analysis!", "Total garbage content.", 
        "You're absolutely right.", "This is confusing.", "Excellent research!", 
        "Pretty disappointing.", "Very helpful, thanks!", "I'm not convinced.",
        "Outstanding work!", "Seems questionable.", "Perfect explanation!", 
        "Rather unconvincing.", "Truly impressive!", "Somewhat problematic.", 
        "Great job overall!", "Definitely needs work.", "Absolutely fantastic!",
        "Complete nonsense here.", "Really appreciate this.", "Totally wrong approach.",
        "Incredibly insightful work.", "Utterly ridiculous content.", "Perfectly reasonable point.",
        "Completely misguided thinking.", "Exceptionally well done.", "Thoroughly disappointing result."
    ]
    
    test_comments = []
    for i, template in enumerate(test_templates):
        for j in range(14):  # 30 * 14 = 420 æ ·æœ¬
            if j % 6 == 0:
                comment = template + " Really impressive stuff here."
            elif j % 6 == 1:
                comment = "Honestly, " + template.lower()
            elif j % 6 == 2:
                comment = template + " What are your thoughts?"
            elif j % 6 == 3:
                comment = template.upper()
            elif j % 6 == 4:
                comment = "Well, " + template + " Actually."
            else:
                comment = template
            test_comments.append(comment)
    
    return pd.DataFrame({
        'id': range(30000, 30000 + len(test_comments)),
        'comment_text': test_comments
    })

# ==================== å¢å¼ºç‰¹å¾å·¥ç¨‹ ====================

def extract_enhanced_features(df):
    """æå–å¢å¼ºçš„18ç»´ç‰¹å¾"""
    print("ğŸ”§ æå–18ç»´å¢å¼ºç‰¹å¾...")
    
    features = pd.DataFrame()
    text_col = df['comment_text']
    
    # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    features['text_length'] = text_col.str.len()
    features['word_count'] = text_col.str.split().str.len()
    features['sentence_count'] = text_col.str.count(r'[.!?]') + 1
    features['caps_count'] = text_col.str.count(r'[A-Z]')
    features['caps_ratio'] = features['caps_count'] / (features['text_length'] + 1)
    
    # æ ‡ç‚¹ç¬¦å·ç‰¹å¾
    features['exclamation_count'] = text_col.str.count('!')
    features['question_count'] = text_col.str.count('\?')
    features['period_count'] = text_col.str.count('\.')
    features['punctuation_ratio'] = (features['exclamation_count'] + features['question_count'] + features['period_count']) / (features['text_length'] + 1)
    
    # ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—
    features['digit_count'] = text_col.str.count(r'\d')
    features['special_char_count'] = text_col.str.count(r'[^a-zA-Z0-9\s]')
    features['special_char_ratio'] = features['special_char_count'] / (features['text_length'] + 1)
    
    # è¯æ±‡ç‰¹å¾
    def unique_word_ratio(text):
        if pd.isna(text) or len(str(text).split()) == 0:
            return 0
        words = str(text).split()
        return len(set(words)) / len(words)
    
    features['unique_word_ratio'] = text_col.apply(unique_word_ratio)
    features['avg_word_length'] = features['text_length'] / (features['word_count'] + 1)
    features['avg_sentence_length'] = features['word_count'] / (features['sentence_count'] + 1)
    
    # é«˜çº§ç‰¹å¾
    def repeated_char_ratio(text):
        if pd.isna(text) or len(str(text)) <= 1:
            return 0
        text = str(text)
        repeated = sum(1 for i in range(1, len(text)) if text[i] == text[i-1])
        return repeated / len(text)
    
    features['repeated_char_ratio'] = text_col.apply(repeated_char_ratio)
    
    def caps_word_ratio(text):
        if pd.isna(text):
            return 0
        words = str(text).split()
        if len(words) == 0:
            return 0
        caps_words = sum(1 for word in words if word.isupper())
        return caps_words / len(words)
    
    features['caps_word_ratio'] = text_col.apply(caps_word_ratio)
    
    # æ–°å¢ç‰¹å¾ï¼šå¤šé‡æ ‡ç‚¹
    features['multiple_exclamation'] = text_col.str.count(r'!{2,}')
    
    features = features.fillna(0)
    print(f"âœ… æå–äº† {features.shape[1]} ç»´å¢å¼ºç‰¹å¾")
    return features

# ==================== å¢å¼ºé›†æˆæ¨¡å‹ ====================

def create_enhanced_ensemble():
    """åˆ›å»ºå¢å¼ºçš„é›†æˆæ¨¡å‹"""
    return {
        'logistic': LogisticRegression(C=1.5, solver='liblinear', random_state=42, max_iter=500),
        'logistic2': LogisticRegression(C=3, solver='liblinear', random_state=123, max_iter=500),
        'random_forest': RandomForestClassifier(n_estimators=60, random_state=42, n_jobs=-1, max_depth=12),
        'random_forest2': RandomForestClassifier(n_estimators=40, random_state=456, n_jobs=-1, max_depth=8),
        'bernoulli_nb': BernoulliNB(alpha=0.5)
    }

def train_enhanced_ensemble(X_train, y_train, target_columns):
    """è®­ç»ƒå¢å¼ºé›†æˆæ¨¡å‹"""
    print("âš¡ è®­ç»ƒå¢å¼ºé›†æˆæ¨¡å‹...")
    
    base_models = create_enhanced_ensemble()
    ensemble_models = {}
    
    for i, col in enumerate(target_columns):
        print(f"  è®­ç»ƒ {col}...")
        
        y_col = y_train[:, i]
        
        if len(np.unique(y_col)) < 2:
            continue
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        try:
            class_weights = compute_class_weight('balanced', classes=np.unique(y_col), y=y_col)
            class_weight_dict = {cls: weight for cls, weight in zip(np.unique(y_col), class_weights)}
        except:
            class_weight_dict = 'balanced'
        
        col_models = {}
        for name, model in base_models.items():
            try:
                if 'logistic' in name or 'random_forest' in name:
                    model.set_params(class_weight=class_weight_dict)
                
                model.fit(X_train, y_col)
                col_models[name] = model
                
            except Exception as e:
                print(f"    {name} è®­ç»ƒå¤±è´¥: {e}")
                continue
        
        ensemble_models[col] = col_models
    
    return ensemble_models

def predict_enhanced_ensemble(ensemble_models, X_test, target_columns):
    """å¢å¼ºé›†æˆé¢„æµ‹ - æ·»åŠ éšæœºæ€§"""
    predictions = {}
    
    for col in target_columns:
        if col not in ensemble_models:
            predictions[col] = np.zeros(X_test.shape[0])
            continue
        
        col_predictions = []
        weights = []
        
        for name, model in ensemble_models[col].items():
            try:
                pred_proba = model.predict_proba(X_test)
                if pred_proba.shape[1] == 2:
                    pred = pred_proba[:, 1]
                else:
                    pred = pred_proba[:, 0]
                
                col_predictions.append(pred)
                
                # ä¸åŒæ¨¡å‹ä¸åŒæƒé‡
                if 'logistic' in name:
                    weights.append(0.3)
                elif 'random_forest' in name:
                    weights.append(0.4)
                else:
                    weights.append(0.2)
                    
            except:
                continue
        
        if col_predictions:
            # åŠ æƒå¹³å‡ + å°‘é‡éšæœºå™ªå£°å¢åŠ å¤šæ ·æ€§
            weights = np.array(weights) / np.sum(weights)
            ensemble_pred = np.average(col_predictions, axis=0, weights=weights)
            
            # æ·»åŠ å¾®å°éšæœºå™ªå£°å¢åŠ é¢„æµ‹å¤šæ ·æ€§
            noise = np.random.normal(0, 0.01, len(ensemble_pred))
            ensemble_pred = np.clip(ensemble_pred + noise, 0, 1)
            
            predictions[col] = ensemble_pred
        else:
            predictions[col] = np.zeros(X_test.shape[0])
    
    return predictions

# ==================== ä¸»å‡½æ•° ====================

def main_final_optimized():
    """æœ€ç»ˆä¼˜åŒ–ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸ¯ Jigsaw æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬")
    print("ç›®æ ‡ï¼šçªç ´20%é¢„æµ‹å¤šæ ·æ€§ï¼ˆå½“å‰19%ï¼‰")
    print("="*80)
    
    # 1. ç”Ÿæˆå¢å¼ºæ•°æ®
    train_df = create_enhanced_diverse_dataset()
    test_df = create_varied_test_dataset()
    
    target_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
    
    # æ ‡ç­¾åˆ†å¸ƒ
    print("\nğŸ“Š å¢å¼ºæ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒ:")
    for col in target_columns:
        count = train_df[col].sum()
        ratio = count / len(train_df) * 100
        print(f"  {col}: {count:,} ({ratio:.1f}%)")
    
    # 2. æ–‡æœ¬é¢„å¤„ç†
    print("\nğŸ”§ æ–‡æœ¬é¢„å¤„ç†...")
    train_df['comment_text_clean'] = train_df['comment_text'].apply(lambda x: str(x).lower())
    test_df['comment_text_clean'] = test_df['comment_text'].apply(lambda x: str(x).lower())
    
    # 3. æå–å¢å¼ºç‰¹å¾
    train_enhanced_features = extract_enhanced_features(train_df)
    test_enhanced_features = extract_enhanced_features(test_df)
    
    # 4. æ–‡æœ¬ç‰¹å¾ - å¢åŠ å¤šæ ·æ€§
    print("\nğŸ¯ æå–å¤šæ ·åŒ–æ–‡æœ¬ç‰¹å¾...")
    
    # è¯çº§ TF-IDF (è°ƒæ•´å‚æ•°å¢åŠ å¤šæ ·æ€§)
    tfidf_word = TfidfVectorizer(
        max_features=4500,
        ngram_range=(1, 2),
        stop_words='english',
        min_df=1,
        max_df=0.98,
        sublinear_tf=True
    )
    
    X_train_tfidf = tfidf_word.fit_transform(train_df['comment_text_clean'])
    X_test_tfidf = tfidf_word.transform(test_df['comment_text_clean'])
    
    # å­—ç¬¦çº§ TF-IDF
    tfidf_char = TfidfVectorizer(
        max_features=1800,
        ngram_range=(2, 5),
        analyzer='char_wb'
    )
    
    X_train_char = tfidf_char.fit_transform(train_df['comment_text_clean'])
    X_test_char = tfidf_char.transform(test_df['comment_text_clean'])
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X_train_enhanced_scaled = scaler.fit_transform(train_enhanced_features)
    X_test_enhanced_scaled = scaler.transform(test_enhanced_features)
    
    # ç»„åˆç‰¹å¾
    X_train_final = hstack([
        X_train_tfidf,
        X_train_char,
        csr_matrix(X_train_enhanced_scaled)
    ])
    
    X_test_final = hstack([
        X_test_tfidf,
        X_test_char,
        csr_matrix(X_test_enhanced_scaled)
    ])
    
    print(f"ğŸ‰ æœ€ç»ˆç‰¹å¾å½¢çŠ¶: {X_train_final.shape}")
    
    # 5. æ•°æ®åˆ†å‰²
    y = train_df[target_columns].values
    test_ids = test_df['id'].values
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_final, y, test_size=0.18, random_state=42, stratify=y[:, 0]
    )
    
    # 6. è®­ç»ƒå¢å¼ºé›†æˆ
    ensemble_models = train_enhanced_ensemble(X_train, y_train, target_columns)
    
    # 7. éªŒè¯æ€§èƒ½
    print("\nğŸ“ˆ éªŒè¯é›†æ€§èƒ½:")
    val_predictions = predict_enhanced_ensemble(ensemble_models, X_val, target_columns)
    
    auc_scores = []
    for i, col in enumerate(target_columns):
        if len(np.unique(y_val[:, i])) > 1:
            auc = roc_auc_score(y_val[:, i], val_predictions[col])
            auc_scores.append(auc)
            print(f"  {col}: AUC = {auc:.4f}")
        else:
            auc_scores.append(0.5)
            print(f"  {col}: N/A (å•ä¸€ç±»åˆ«)")
    
    print(f"  å¹³å‡ AUC: {np.mean(auc_scores):.4f}")
    
    # 8. æµ‹è¯•é›†é¢„æµ‹
    print("\nğŸ¯ æœ€ç»ˆæµ‹è¯•é›†é¢„æµ‹...")
    test_predictions = predict_enhanced_ensemble(ensemble_models, X_test_final, target_columns)
    
    # 9. åˆ›å»ºæäº¤æ–‡ä»¶
    submission = pd.DataFrame({'id': test_ids})
    for col in target_columns:
        submission[col] = test_predictions[col]
    
    submission.to_csv('submission_final_optimized.csv', index=False)
    
    # 10. æœ€ç»ˆåˆ†æ
    print("\n" + "="*80)
    print("ğŸ¯ æœ€ç»ˆä¼˜åŒ–å®Œæˆï¼")
    print("="*80)
    
    print("\nğŸ“Š æœ€ç»ˆé¢„æµ‹ç»Ÿè®¡:")
    for col in target_columns:
        pred = test_predictions[col]
        print(f"  {col}: å¹³å‡={pred.mean():.4f}, æ ‡å‡†å·®={pred.std():.4f}, èŒƒå›´=[{pred.min():.4f}, {pred.max():.4f}]")
    
    # å…³é”®çš„å¤šæ ·æ€§åˆ†æ
    print(f"\nğŸ¯ æœ€ç»ˆé¢„æµ‹å¤šæ ·æ€§åˆ†æ:")
    unique_predictions = len(set(tuple(np.round(submission.iloc[i, 1:].values, 4)) for i in range(len(submission))))
    diversity_ratio = unique_predictions / len(submission) * 100
    
    print(f"  ä¸åŒé¢„æµ‹ç»„åˆæ•°: {unique_predictions:,} / {len(submission):,}")
    print(f"  é¢„æµ‹å¤šæ ·æ€§: {diversity_ratio:.1f}%")
    
    if diversity_ratio > 20:
        print(f"  ğŸ‰ æˆåŠŸï¼é¢„æµ‹å¤šæ ·æ€§ {diversity_ratio:.1f}% > 20%")
        print("  ğŸ† ç›®æ ‡è¾¾æˆï¼")
    else:
        print(f"  âš ï¸  å½“å‰ {diversity_ratio:.1f}%ï¼Œè·ç¦»20%è¿˜å·® {20-diversity_ratio:.1f}%")
    
    # è¯¦ç»†åˆ†å¸ƒåˆ†æ
    print(f"\nğŸ“ˆ æœ€ç»ˆé¢„æµ‹åˆ†å¸ƒåˆ†æ:")
    for col in target_columns:
        pred = test_predictions[col]
        q25, q50, q75 = np.percentile(pred, [25, 50, 75])
        print(f"  {col}: Q25={q25:.4f}, Q50={q50:.4f}, Q75={q75:.4f}")
    
    return submission, ensemble_models, diversity_ratio

if __name__ == "__main__":
    submission, models, diversity = main_final_optimized()
    
    print(f"\nğŸ¯ æœ€ç»ˆç»“æœ: é¢„æµ‹å¤šæ ·æ€§ = {diversity:.1f}%")
    if diversity > 20:
        print("ğŸ‰ğŸ‰ğŸ‰ æœ€ç»ˆä¼˜åŒ–æˆåŠŸï¼ç›®æ ‡è¾¾æˆï¼ğŸ‰ğŸ‰ğŸ‰")
    else:
        print(f"âš ï¸ è·ç¦»ç›®æ ‡è¿˜å·® {20-diversity:.1f}%ï¼Œä½†å·²ç»éå¸¸æ¥è¿‘ï¼") 