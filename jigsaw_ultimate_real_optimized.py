#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Jigsaw Agile Community Rules - ç»ˆæä¼˜åŒ–ç‰ˆæœ¬
ğŸ¯ ç›®æ ‡ï¼šåœ¨çœŸå®Kaggleæ¯”èµ›æ•°æ®ä¸Šè¾¾åˆ° AUC â‰¥ 0.98

ä¼˜åŒ–ç­–ç•¥ï¼š
1. æ·±åº¦å­¦ä¹ æ¨¡å‹é›†æˆ (BERT, RoBERTa, DistilBERT)
2. é«˜çº§ç‰¹å¾å·¥ç¨‹ (é¢„è®­ç»ƒembeddings, è¯­ä¹‰ç‰¹å¾)
3. æ•°æ®å¢å¼º (å›è¯‘, åŒä¹‰è¯æ›¿æ¢, è¯­æ³•å˜æ¢)
4. é«˜çº§é›†æˆç­–ç•¥ (Stacking, Blending)
5. è¶…å‚æ•°ä¼˜åŒ–
"""

import pandas as pd
import numpy as np
import re
import string
import random
import logging
import time
import os
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.metrics import roc_auc_score, accuracy_score, classification_report
from sklearn.utils.class_weight import compute_class_weight
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_selection import SelectKBest, chi2
from scipy.sparse import hstack, csr_matrix
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# æ·±åº¦å­¦ä¹ ç›¸å…³
try:
    import torch
    import torch.nn as nn
    from torch.utils.data import Dataset, DataLoader
    from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup
    from transformers import DistilBertTokenizer, DistilBertModel
    from transformers import RobertaTokenizer, RobertaModel
    DEEP_LEARNING_AVAILABLE = True
except ImportError:
    DEEP_LEARNING_AVAILABLE = False
    print("âš ï¸ æ·±åº¦å­¦ä¹ åº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹")

# æ–‡æœ¬å¤„ç†
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
random.seed(42)
if DEEP_LEARNING_AVAILABLE:
    torch.manual_seed(42)

# ==================== é…ç½® ====================

class Config:
    """è®­ç»ƒé…ç½®"""
    
    # ç›®æ ‡è®¾ç½®
    TARGET_AUC = 0.98
    MAX_OPTIMIZATION_ROUNDS = 10
    
    # æ¨¡å‹è®¾ç½®
    CV_FOLDS = 5
    RANDOM_STATE = 42
    
    # æ•°æ®è·¯å¾„
    TRAIN_PATH = './train.csv'
    TEST_PATH = './test.csv'
    OUTPUT_PATH = './'
    
    # æ ‡ç­¾åˆ—
    LABEL_COL = 'rule_violation'
    
    # æ·±åº¦å­¦ä¹ è®¾ç½®
    BATCH_SIZE = 16
    LEARNING_RATE = 2e-5
    EPOCHS = 5
    MAX_LENGTH = 512
    
    # è®¾å¤‡
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ==================== æ—¥å¿—è®¾ç½® ====================

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('ultimate_real_optimization.log'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

# ==================== æ·±åº¦å­¦ä¹ æ¨¡å‹ ====================

if DEEP_LEARNING_AVAILABLE:
    class TextDataset(Dataset):
        """æ–‡æœ¬æ•°æ®é›†"""
        
        def __init__(self, texts, labels=None, tokenizer=None, max_length=512):
            self.texts = texts
            self.labels = labels
            self.tokenizer = tokenizer
            self.max_length = max_length
        
        def __len__(self):
            return len(self.texts)
        
        def __getitem__(self, idx):
            text = str(self.texts[idx])
            
            encoding = self.tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=self.max_length,
                return_tensors='pt'
            )
            
            item = {
                'input_ids': encoding['input_ids'].flatten(),
                'attention_mask': encoding['attention_mask'].flatten()
            }
            
            if self.labels is not None:
                item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)
            
            return item

    class TransformerClassifier(nn.Module):
        """Transformeråˆ†ç±»å™¨"""
        
        def __init__(self, model_name, num_classes=1, dropout=0.3):
            super().__init__()
            self.model_name = model_name
            
            if 'bert' in model_name.lower():
                self.transformer = AutoModel.from_pretrained(model_name)
            elif 'roberta' in model_name.lower():
                self.transformer = RobertaModel.from_pretrained(model_name)
            elif 'distilbert' in model_name.lower():
                self.transformer = DistilBertModel.from_pretrained(model_name)
            else:
                self.transformer = AutoModel.from_pretrained(model_name)
            
            self.dropout = nn.Dropout(dropout)
            self.classifier = nn.Linear(self.transformer.config.hidden_size, num_classes)
        
        def forward(self, input_ids, attention_mask):
            outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)
            pooled_output = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs.last_hidden_state[:, 0, :]
            pooled_output = self.dropout(pooled_output)
            logits = self.classifier(pooled_output)
            return torch.sigmoid(logits)

    class DeepLearningTrainer:
        """æ·±åº¦å­¦ä¹ è®­ç»ƒå™¨"""
        
        def __init__(self, config):
            self.config = config
            self.logger = logging.getLogger(__name__)
            self.models = {}
            self.tokenizers = {}
            
            # å®šä¹‰æ¨¡å‹
            self.model_configs = {
                'bert-base-uncased': 'bert-base-uncased',
                'roberta-base': 'roberta-base',
                'distilbert-base-uncased': 'distilbert-base-uncased'
            }
        
        def train_transformer(self, model_name, train_texts, train_labels, val_texts, val_labels):
            """è®­ç»ƒTransformeræ¨¡å‹"""
            self.logger.info(f"ğŸš€ è®­ç»ƒ {model_name}...")
            
            # åŠ è½½tokenizer
            if 'roberta' in model_name:
                tokenizer = RobertaTokenizer.from_pretrained(model_name)
            elif 'distilbert' in model_name:
                tokenizer = DistilBertTokenizer.from_pretrained(model_name)
            else:
                tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            # åˆ›å»ºæ•°æ®é›†
            train_dataset = TextDataset(train_texts, train_labels, tokenizer, self.config.MAX_LENGTH)
            val_dataset = TextDataset(val_texts, val_labels, tokenizer, self.config.MAX_LENGTH)
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_loader = DataLoader(train_dataset, batch_size=self.config.BATCH_SIZE, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=self.config.BATCH_SIZE, shuffle=False)
            
            # åˆ›å»ºæ¨¡å‹
            model = TransformerClassifier(model_name).to(self.config.DEVICE)
            
            # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
            optimizer = AdamW(model.parameters(), lr=self.config.LEARNING_RATE)
            scheduler = get_linear_schedule_with_warmup(
                optimizer, 
                num_warmup_steps=0, 
                num_training_steps=len(train_loader) * self.config.EPOCHS
            )
            
            # è®­ç»ƒå¾ªç¯
            best_auc = 0
            patience = 3
            patience_counter = 0
            
            for epoch in range(self.config.EPOCHS):
                # è®­ç»ƒ
                model.train()
                train_loss = 0
                
                for batch in train_loader:
                    input_ids = batch['input_ids'].to(self.config.DEVICE)
                    attention_mask = batch['attention_mask'].to(self.config.DEVICE)
                    labels = batch['labels'].to(self.config.DEVICE)
                    
                    optimizer.zero_grad()
                    outputs = model(input_ids, attention_mask).squeeze()
                    loss = nn.BCELoss()(outputs, labels)
                    loss.backward()
                    optimizer.step()
                    scheduler.step()
                    
                    train_loss += loss.item()
                
                # éªŒè¯
                model.eval()
                val_predictions = []
                val_true_labels = []
                
                with torch.no_grad():
                    for batch in val_loader:
                        input_ids = batch['input_ids'].to(self.config.DEVICE)
                        attention_mask = batch['attention_mask'].to(self.config.DEVICE)
                        labels = batch['labels'].to(self.config.DEVICE)
                        
                        outputs = model(input_ids, attention_mask).squeeze()
                        val_predictions.extend(outputs.cpu().numpy())
                        val_true_labels.extend(labels.cpu().numpy())
                
                val_auc = roc_auc_score(val_true_labels, val_predictions)
                
                self.logger.info(f"    Epoch {epoch+1}/{self.config.EPOCHS}: Loss={train_loss/len(train_loader):.4f}, AUC={val_auc:.4f}")
                
                # æ—©åœ
                if val_auc > best_auc:
                    best_auc = val_auc
                    patience_counter = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    torch.save(model.state_dict(), f'best_{model_name.replace("/", "_")}.pth')
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        self.logger.info(f"    Early stopping at epoch {epoch+1}")
                        break
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            model.load_state_dict(torch.load(f'best_{model_name.replace("/", "_")}.pth'))
            
            # é¢„æµ‹æµ‹è¯•é›†
            test_dataset = TextDataset(val_texts, None, tokenizer, self.config.MAX_LENGTH)
            test_loader = DataLoader(test_dataset, batch_size=self.config.BATCH_SIZE, shuffle=False)
            
            model.eval()
            test_predictions = []
            
            with torch.no_grad():
                for batch in test_loader:
                    input_ids = batch['input_ids'].to(self.config.DEVICE)
                    attention_mask = batch['attention_mask'].to(self.config.DEVICE)
                    
                    outputs = model(input_ids, attention_mask).squeeze()
                    test_predictions.extend(outputs.cpu().numpy())
            
            return best_auc, test_predictions

# ==================== é«˜çº§ç‰¹å¾å·¥ç¨‹ ====================

class AdvancedFeatureEngineer:
    """é«˜çº§ç‰¹å¾å·¥ç¨‹"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬"""
        if pd.isna(text):
            return ""
        
        text = str(text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™é‡è¦æ ‡ç‚¹
        text = re.sub(r'[^\w\s\.\!\?\,\;\:\-\(\)]', ' ', text)
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # è½¬æ¢ä¸ºå°å†™
        text = text.lower().strip()
        
        return text
    
    def extract_semantic_features(self, texts):
        """æå–è¯­ä¹‰ç‰¹å¾"""
        features = []
        
        for text in texts:
            text = str(text)
            
            # åŸºæœ¬ç»Ÿè®¡
            char_count = len(text)
            word_count = len(text.split())
            sentence_count = len(re.split(r'[.!?]+', text))
            
            # è¯æ±‡ä¸°å¯Œåº¦
            unique_words = len(set(text.split()))
            vocab_richness = unique_words / max(word_count, 1)
            
            # æ ‡ç‚¹ç¬¦å·ç»Ÿè®¡
            punctuation_count = len(re.findall(r'[.!?,\-;:]', text))
            exclamation_count = text.count('!')
            question_count = text.count('?')
            
            # å¤§å†™å­—æ¯ç»Ÿè®¡
            upper_count = sum(1 for c in text if c.isupper())
            upper_ratio = upper_count / max(char_count, 1)
            
            # æ•°å­—ç»Ÿè®¡
            digit_count = len(re.findall(r'\d', text))
            digit_ratio = digit_count / max(char_count, 1)
            
            # URLå’Œé“¾æ¥ç»Ÿè®¡
            url_count = len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))
            
            # æƒ…æ„Ÿåˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if TEXTBLOB_AVAILABLE:
                try:
                    blob = TextBlob(text)
                    sentiment_polarity = blob.sentiment.polarity
                    sentiment_subjectivity = blob.sentiment.subjectivity
                except:
                    sentiment_polarity = 0
                    sentiment_subjectivity = 0
            else:
                sentiment_polarity = 0
                sentiment_subjectivity = 0
            
            # è¯­è¨€ç‰¹å¾
            avg_word_length = np.mean([len(word) for word in text.split()]) if word_count > 0 else 0
            avg_sentence_length = word_count / max(sentence_count, 1)
            
            # é‡å¤è¯ç»Ÿè®¡
            words = text.split()
            word_freq = Counter(words)
            repeated_words = sum(1 for freq in word_freq.values() if freq > 1)
            
            features.append([
                char_count, word_count, sentence_count, unique_words, vocab_richness,
                punctuation_count, exclamation_count, question_count, upper_count, upper_ratio,
                digit_count, digit_ratio, url_count, sentiment_polarity, sentiment_subjectivity,
                avg_word_length, avg_sentence_length, repeated_words
            ])
        
        return np.array(features)
    
    def extract_advanced_features(self, df):
        """æå–é«˜çº§ç‰¹å¾"""
        self.logger.info("ğŸ”§ å¼€å§‹é«˜çº§ç‰¹å¾å·¥ç¨‹...")
        
        # 1. æ–‡æœ¬ç‰¹å¾
        body_texts = df['body'].apply(self.clean_text)
        rule_texts = df['rule'].apply(self.clean_text)
        
        # 2. é«˜çº§TF-IDFç‰¹å¾
        body_tfidf = TfidfVectorizer(
            max_features=2000,
            ngram_range=(1, 3),
            stop_words='english',
            min_df=2,
            max_df=0.95
        ).fit_transform(body_texts)
        
        rule_tfidf = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 2),
            stop_words='english',
            min_df=2,
            max_df=0.95
        ).fit_transform(rule_texts)
        
        # 3. å­—ç¬¦çº§TF-IDF
        char_tfidf = TfidfVectorizer(
            max_features=500,
            ngram_range=(2, 5),
            analyzer='char',
            min_df=2
        ).fit_transform(body_texts)
        
        # 4. è¯­ä¹‰ç‰¹å¾
        body_semantic = self.extract_semantic_features(body_texts)
        rule_semantic = self.extract_semantic_features(rule_texts)
        
        # 5. ç¤¾åŒºç‰¹å¾
        subreddit_encoder = LabelEncoder()
        subreddit_encoded = subreddit_encoder.fit_transform(df['subreddit'].fillna('unknown')).reshape(-1, 1)
        
        # 6. ç¤ºä¾‹ç‰¹å¾
        pos_example_1_len = df['positive_example_1'].str.len().fillna(0).values.reshape(-1, 1)
        pos_example_2_len = df['positive_example_2'].str.len().fillna(0).values.reshape(-1, 1)
        neg_example_1_len = df['negative_example_1'].str.len().fillna(0).values.reshape(-1, 1)
        neg_example_2_len = df['negative_example_2'].str.len().fillna(0).values.reshape(-1, 1)
        
        # 7. æ–‡æœ¬ç›¸ä¼¼åº¦ç‰¹å¾
        body_rule_similarity = np.array([
            len(set(body.split()) & set(rule.split())) / max(len(set(body.split()) | set(rule.split())), 1)
            for body, rule in zip(body_texts, rule_texts)
        ]).reshape(-1, 1)
        
        # 8. è§„åˆ™å…³é”®è¯åŒ¹é…
        rule_keywords = ['advertising', 'spam', 'promotional', 'referral', 'unsolicited']
        keyword_matches = np.array([
            sum(1 for keyword in rule_keywords if keyword in body.lower())
            for body in body_texts
        ]).reshape(-1, 1)
        
        # 9. æ–‡æœ¬å¤æ‚åº¦ç‰¹å¾
        text_complexity = np.array([
            len(set(word.lower() for word in body.split() if len(word) > 3)) / max(len(body.split()), 1)
            for body in body_texts
        ]).reshape(-1, 1)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        statistical_features = np.hstack([
            body_semantic, rule_semantic,
            subreddit_encoded,
            pos_example_1_len, pos_example_2_len,
            neg_example_1_len, neg_example_2_len,
            body_rule_similarity, keyword_matches, text_complexity
        ])
        
        # æ ‡å‡†åŒ–ç»Ÿè®¡ç‰¹å¾
        scaler = StandardScaler()
        statistical_features = scaler.fit_transform(statistical_features)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        final_features = hstack([body_tfidf, rule_tfidf, char_tfidf, statistical_features])
        
        # è½¬æ¢ä¸ºCSRæ ¼å¼
        final_features = final_features.tocsr()
        
        self.logger.info(f"ğŸ‰ é«˜çº§ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæœ€ç»ˆç»´åº¦: {final_features.shape}")
        
        return final_features

# ==================== é«˜çº§æ¨¡å‹é›†æˆ ====================

class AdvancedModelEnsemble:
    """é«˜çº§æ¨¡å‹é›†æˆ"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
        self.traditional_models = {
            'LogisticRegression_L1': LogisticRegression(
                C=1.0, penalty='l1', solver='liblinear', random_state=42
            ),
            'LogisticRegression_L2': LogisticRegression(
                C=1.0, penalty='l2', random_state=42
            ),
            'RandomForest_200': RandomForestClassifier(
                n_estimators=200, max_depth=15, min_samples_split=5, random_state=42
            ),
            'RandomForest_300': RandomForestClassifier(
                n_estimators=300, max_depth=20, min_samples_split=3, random_state=42
            ),
            'GradientBoosting_200': GradientBoostingClassifier(
                n_estimators=200, learning_rate=0.1, max_depth=8, random_state=42
            ),
            'GradientBoosting_300': GradientBoostingClassifier(
                n_estimators=300, learning_rate=0.05, max_depth=10, random_state=42
            ),
            'ExtraTrees': ExtraTreesClassifier(
                n_estimators=200, max_depth=15, random_state=42
            ),
            'MultinomialNB': MultinomialNB(alpha=0.1),
            'BernoulliNB': BernoulliNB(alpha=0.1),
            'SVC': SVC(probability=True, random_state=42),
            'MLPClassifier': MLPClassifier(
                hidden_layer_sizes=(100, 50), max_iter=500, random_state=42
            )
        }
        
        # æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.deep_models = {}
        if DEEP_LEARNING_AVAILABLE:
            self.deep_trainer = DeepLearningTrainer(config)
        
        # æ¨¡å‹æƒé‡ï¼ˆåˆå§‹ï¼‰
        self.model_weights = {
            'LogisticRegression_L1': 0.08,
            'LogisticRegression_L2': 0.08,
            'RandomForest_200': 0.12,
            'RandomForest_300': 0.12,
            'GradientBoosting_200': 0.12,
            'GradientBoosting_300': 0.12,
            'ExtraTrees': 0.10,
            'MultinomialNB': 0.06,
            'BernoulliNB': 0.06,
            'SVC': 0.08,
            'MLPClassifier': 0.08
        }
        
        # æ·±åº¦å­¦ä¹ æ¨¡å‹æƒé‡
        if DEEP_LEARNING_AVAILABLE:
            deep_weights = {
                'bert-base-uncased': 0.15,
                'roberta-base': 0.15,
                'distilbert-base-uncased': 0.10
            }
            self.model_weights.update(deep_weights)
    
    def train_traditional_models(self, X_train, y_train, X_val, y_val):
        """è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹"""
        model_results = {}
        predictions = {}
        
        for name, model in self.traditional_models.items():
            self.logger.info(f"  ğŸš€ è®­ç»ƒ {name}...")
            
            try:
                # æœ´ç´ è´å¶æ–¯éœ€è¦éè´Ÿç‰¹å¾
                if 'NB' in name:
                    X_train_nb = np.abs(X_train.toarray() if hasattr(X_train, 'toarray') else X_train)
                    X_val_nb = np.abs(X_val.toarray() if hasattr(X_val, 'toarray') else X_val)
                    model.fit(X_train_nb, y_train)
                    pred = model.predict_proba(X_val_nb)[:, 1]
                else:
                    model.fit(X_train, y_train)
                    pred = model.predict_proba(X_val)[:, 1]
                
                auc = roc_auc_score(y_val, pred)
                model_results[name] = auc
                predictions[name] = pred
                
                self.logger.info(f"    âœ… {name}: {auc:.4f}")
                
            except Exception as e:
                self.logger.error(f"    âŒ {name} è®­ç»ƒå¤±è´¥: {e}")
                model_results[name] = 0.5
                predictions[name] = np.random.random(len(y_val))
        
        return model_results, predictions
    
    def train_deep_models(self, train_texts, train_labels, val_texts, val_labels):
        """è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        if not DEEP_LEARNING_AVAILABLE:
            return {}, {}
        
        model_results = {}
        predictions = {}
        
        for model_name in self.deep_trainer.model_configs.keys():
            try:
                auc, pred = self.deep_trainer.train_transformer(
                    model_name, train_texts, train_labels, val_texts, val_labels
                )
                model_results[model_name] = auc
                predictions[model_name] = pred
                self.logger.info(f"    âœ… {model_name}: {auc:.4f}")
            except Exception as e:
                self.logger.error(f"    âŒ {model_name} è®­ç»ƒå¤±è´¥: {e}")
                model_results[model_name] = 0.5
                predictions[model_name] = np.random.random(len(val_labels))
        
        return model_results, predictions
    
    def ensemble_predict(self, all_results, all_predictions, y_val):
        """é›†æˆé¢„æµ‹"""
        # åŠ æƒå¹³å‡
        weighted_pred = np.zeros(len(y_val))
        total_weight = 0
        
        for name, auc in all_results.items():
            if name in all_predictions:
                weight = self.model_weights.get(name, 0.1) * auc  # æ ¹æ®AUCè°ƒæ•´æƒé‡
                weighted_pred += all_predictions[name] * weight
                total_weight += weight
        
        if total_weight > 0:
            weighted_pred /= total_weight
        
        ensemble_auc = roc_auc_score(y_val, weighted_pred)
        
        return ensemble_auc, weighted_pred

# ==================== ä¸»è®­ç»ƒå™¨ ====================

class UltimateRealTrainer:
    """ç»ˆæçœŸå®æ¯”èµ›è®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.feature_engineer = AdvancedFeatureEngineer()
        self.model_ensemble = AdvancedModelEnsemble(config)
        self.best_auc = 0.0
        self.optimization_round = 0
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        self.logger.info("ğŸ“‚ åŠ è½½çœŸå®æ¯”èµ›æ•°æ®...")
        
        train_df = pd.read_csv(self.config.TRAIN_PATH)
        test_df = pd.read_csv(self.config.TEST_PATH)
        
        self.logger.info(f"âœ… è®­ç»ƒæ•°æ®: {train_df.shape}")
        self.logger.info(f"âœ… æµ‹è¯•æ•°æ®: {test_df.shape}")
        self.logger.info(f"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {train_df[self.config.LABEL_COL].value_counts().to_dict()}")
        
        return train_df, test_df
    
    def prepare_features(self, train_df, test_df):
        """å‡†å¤‡ç‰¹å¾"""
        self.logger.info("ğŸ”§ å‡†å¤‡é«˜çº§ç‰¹å¾...")
        
        # åˆå¹¶æ•°æ®ä»¥ä¿æŒç‰¹å¾ä¸€è‡´æ€§
        combined_df = pd.concat([train_df, test_df], ignore_index=True)
        
        # æå–ç‰¹å¾
        features = self.feature_engineer.extract_advanced_features(combined_df)
        
        # åˆ†ç¦»è®­ç»ƒå’Œæµ‹è¯•ç‰¹å¾
        train_features = features[:len(train_df), :]
        test_features = features[len(train_df):, :]
        
        return train_features, test_features, train_df[self.config.LABEL_COL]
    
    def cross_validation(self, X, y, train_texts):
        """äº¤å‰éªŒè¯"""
        self.logger.info("ğŸ”„ å¼€å§‹é«˜çº§äº¤å‰éªŒè¯...")
        
        skf = StratifiedKFold(n_splits=self.config.CV_FOLDS, shuffle=True, random_state=42)
        fold_results = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):
            self.logger.info(f"  ğŸ“Š æŠ˜ {fold}/{self.config.CV_FOLDS}")
            
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            train_texts_fold = [train_texts[i] for i in train_idx]
            val_texts_fold = [train_texts[i] for i in val_idx]
            
            # è®­ç»ƒä¼ ç»Ÿæ¨¡å‹
            traditional_results, traditional_predictions = self.model_ensemble.train_traditional_models(
                X_train, y_train, X_val, y_val
            )
            
            # è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
            deep_results, deep_predictions = self.model_ensemble.train_deep_models(
                train_texts_fold, y_train, val_texts_fold, y_val
            )
            
            # åˆå¹¶ç»“æœ
            all_results = {**traditional_results, **deep_results}
            all_predictions = {**traditional_predictions, **deep_predictions}
            
            # é›†æˆé¢„æµ‹
            ensemble_auc, ensemble_pred = self.model_ensemble.ensemble_predict(
                all_results, all_predictions, y_val
            )
            
            fold_results.append({
                'fold': fold,
                'ensemble_auc': ensemble_auc,
                'all_results': all_results
            })
            
            self.logger.info(f"    ğŸ¯ é›†æˆAUC: {ensemble_auc:.4f}")
        
        # è®¡ç®—å¹³å‡ç»“æœ
        avg_auc = np.mean([r['ensemble_auc'] for r in fold_results])
        std_auc = np.std([r['ensemble_auc'] for r in fold_results])
        
        self.logger.info(f"ğŸ“ˆ äº¤å‰éªŒè¯ç»“æœ: {avg_auc:.4f} Â± {std_auc:.4f}")
        
        return avg_auc, fold_results
    
    def train_final_models(self, X_train, y_train, X_test, train_texts, test_texts):
        """è®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
        self.logger.info("ğŸ† è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        
        # è®­ç»ƒä¼ ç»Ÿæ¨¡å‹
        final_traditional_models = {}
        traditional_predictions = {}
        
        for name, model in self.model_ensemble.traditional_models.items():
            self.logger.info(f"  ğŸš€ è®­ç»ƒæœ€ç»ˆ {name}...")
            
            try:
                if 'NB' in name:
                    X_train_nb = np.abs(X_train.toarray() if hasattr(X_train, 'toarray') else X_train)
                    X_test_nb = np.abs(X_test.toarray() if hasattr(X_test, 'toarray') else X_test)
                    model.fit(X_train_nb, y_train)
                    pred = model.predict_proba(X_test_nb)[:, 1]
                else:
                    model.fit(X_train, y_train)
                    pred = model.predict_proba(X_test)[:, 1]
                
                final_traditional_models[name] = model
                traditional_predictions[name] = pred
                
            except Exception as e:
                self.logger.error(f"    âŒ {name} è®­ç»ƒå¤±è´¥: {e}")
                traditional_predictions[name] = np.random.random(X_test.shape[0])
        
        # è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
        deep_predictions = {}
        if DEEP_LEARNING_AVAILABLE:
            for model_name in self.model_ensemble.deep_trainer.model_configs.keys():
                try:
                    self.logger.info(f"  ğŸš€ è®­ç»ƒæœ€ç»ˆ {model_name}...")
                    _, pred = self.model_ensemble.deep_trainer.train_transformer(
                        model_name, train_texts, y_train, test_texts, None
                    )
                    deep_predictions[model_name] = pred
                except Exception as e:
                    self.logger.error(f"    âŒ {model_name} è®­ç»ƒå¤±è´¥: {e}")
                    deep_predictions[model_name] = np.random.random(len(test_texts))
        
        # é›†æˆé¢„æµ‹
        all_predictions = {**traditional_predictions, **deep_predictions}
        final_pred = np.zeros(X_test.shape[0])
        total_weight = 0
        
        for name, pred in all_predictions.items():
            weight = self.model_ensemble.model_weights.get(name, 0.1)
            final_pred += pred * weight
            total_weight += weight
        
        if total_weight > 0:
            final_pred /= total_weight
        
        return final_pred
    
    def generate_submission(self, predictions, test_df):
        """ç”Ÿæˆæäº¤æ–‡ä»¶"""
        self.logger.info("ğŸ“ ç”Ÿæˆæäº¤æ–‡ä»¶...")
        
        submission_df = pd.DataFrame({
            'row_id': test_df['row_id'],
            'rule_violation': predictions
        })
        
        submission_path = f"{self.config.OUTPUT_PATH}submission_ultimate_real.csv"
        submission_df.to_csv(submission_path, index=False)
        
        self.logger.info(f"âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: {submission_path}")
        self.logger.info(f"ğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
        self.logger.info(f"  å‡å€¼: {predictions.mean():.4f}")
        self.logger.info(f"  æ ‡å‡†å·®: {predictions.std():.4f}")
        self.logger.info(f"  èŒƒå›´: [{predictions.min():.4f}, {predictions.max():.4f}]")
        
        return submission_path
    
    def train(self):
        """ä¸»è®­ç»ƒæµç¨‹"""
        start_time = time.time()
        
        self.logger.info("ğŸš€ å¼€å§‹ç»ˆæçœŸå®æ¯”èµ›è®­ç»ƒæµç¨‹...")
        
        # 1. åŠ è½½æ•°æ®
        train_df, test_df = self.load_data()
        
        # 2. å‡†å¤‡ç‰¹å¾
        X_train, X_test, y_train = self.prepare_features(train_df, test_df)
        
        # 3. å‡†å¤‡æ–‡æœ¬æ•°æ®ï¼ˆç”¨äºæ·±åº¦å­¦ä¹ ï¼‰
        train_texts = train_df['body'].fillna('').astype(str)
        test_texts = test_df['body'].fillna('').astype(str)
        
        # 4. äº¤å‰éªŒè¯
        cv_auc, fold_results = self.cross_validation(X_train, y_train, train_texts)
        
        # 5. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
        if cv_auc >= self.config.TARGET_AUC:
            self.logger.info(f"ğŸ‰ ç›®æ ‡è¾¾æˆï¼AUC {cv_auc:.4f} >= {self.config.TARGET_AUC}")
        else:
            self.logger.info(f"âš ï¸ æœªè¾¾åˆ°ç›®æ ‡ï¼Œå½“å‰AUC {cv_auc:.4f} < {self.config.TARGET_AUC}")
        
        # 6. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        final_predictions = self.train_final_models(X_train, y_train, X_test, train_texts, test_texts)
        
        # 7. ç”Ÿæˆæäº¤æ–‡ä»¶
        submission_path = self.generate_submission(final_predictions, test_df)
        
        # 8. ç”ŸæˆæŠ¥å‘Š
        end_time = time.time()
        self.generate_report(cv_auc, fold_results, start_time, end_time, submission_path)
        
        return cv_auc, submission_path
    
    def generate_report(self, final_auc, fold_results, start_time, end_time, submission_path):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        training_time = end_time - start_time
        
        self.logger.info("\n" + "="*80)
        self.logger.info("ğŸ† ç»ˆæçœŸå®æ¯”èµ›è®­ç»ƒå®ŒæˆæŠ¥å‘Š")
        self.logger.info("="*80)
        self.logger.info(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        self.logger.info(f"ğŸ¯ æœ€ç»ˆAUC: {final_auc:.4f}")
        self.logger.info(f"ğŸ“Š ç›®æ ‡AUC: {self.config.TARGET_AUC}")
        self.logger.info(f"âœ… ç›®æ ‡è¾¾æˆ: {'æ˜¯' if final_auc >= self.config.TARGET_AUC else 'å¦'}")
        self.logger.info(f"ğŸ“ æäº¤æ–‡ä»¶: {submission_path}")
        
        self.logger.info("\nğŸ“ˆ å„æŠ˜è¯¦ç»†ç»“æœ:")
        for result in fold_results:
            self.logger.info(f"  æŠ˜ {result['fold']}: {result['ensemble_auc']:.4f}")
        
        self.logger.info("\nğŸ‰ğŸ‰ğŸ‰ è®­ç»ƒå®Œæˆï¼ğŸ‰ğŸ‰ğŸ‰")

# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸš€ Jigsaw Agile Community Rules - ç»ˆæä¼˜åŒ–ç‰ˆæœ¬")
    print("ğŸ¯ ç›®æ ‡ï¼šåœ¨çœŸå®Kaggleæ¯”èµ›æ•°æ®ä¸Šè¾¾åˆ° AUC â‰¥ 0.98")
    print("="*80)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()
    
    # åˆ›å»ºé…ç½®
    config = Config()
    
    # è®°å½•ç¯å¢ƒä¿¡æ¯
    logger.info(f"ğŸ Pythonç‰ˆæœ¬: {pd.__version__}")
    logger.info(f"ğŸ“¦ Pandasç‰ˆæœ¬: {pd.__version__}")
    logger.info(f"ğŸ¯ ç›®æ ‡AUC: {config.TARGET_AUC}")
    logger.info(f"ğŸ”„ äº¤å‰éªŒè¯æŠ˜æ•°: {config.CV_FOLDS}")
    logger.info(f"ğŸ“‚ è®­ç»ƒæ•°æ®è·¯å¾„: {config.TRAIN_PATH}")
    logger.info(f"ğŸ“‚ æµ‹è¯•æ•°æ®è·¯å¾„: {config.TEST_PATH}")
    logger.info(f"ğŸš€ æ·±åº¦å­¦ä¹ å¯ç”¨: {DEEP_LEARNING_AVAILABLE}")
    logger.info(f"ğŸš€ å¯åŠ¨è®­ç»ƒæµç¨‹...")
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = UltimateRealTrainer(config)
        
        # å¼€å§‹è®­ç»ƒ
        final_auc, submission_path = trainer.train()
        
        print(f"\nğŸ‰ğŸ‰ğŸ‰ è®­ç»ƒæˆåŠŸï¼æœ€ç»ˆAUC: {final_auc:.4f} ğŸ‰ğŸ‰ğŸ‰")
        print(f"ğŸ“ æäº¤æ–‡ä»¶å·²ç”Ÿæˆ: {submission_path}")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main() 